{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Intro\n",
    "\n",
    "#### Properties:\n",
    "\n",
    "-   VERY nonlinear, non-parametric\n",
    "-   Input layer: the data you pass\n",
    "-   Hidden layers: transform the data and extract high-level features\n",
    "    -   Biases, weights, & activation functions are applied to nodes\n",
    "        (data points), which are selectively passed to next layer.\n",
    "        Transforming nodes as such gives new information as we go layer\n",
    "        to layer (like a conveyer belt)\n",
    "    -   **AKA forward propogation**:\n",
    "    -   EX: let’s say we were observing a toy factory that makes rocking\n",
    "        horses and model trucks. At the beginning of their conveyor,\n",
    "        we’d see raw wood formed into separate pieces, then decorated\n",
    "        and painted, then assembled; As we go layer by layer, we gain\n",
    "        additional information about our final object. The information\n",
    "        we gather - texture, shape, colors, etc - are the ‘features’ of\n",
    "        our data\n",
    "    -   Deep learning models are referred to as “black box” because we\n",
    "        ask our models to determine them for us (like if we set our\n",
    "        assembly line to ‘auto- pilot’). The model estimates millions of\n",
    "        parameters (weights/biases applied to nodes), which gives DL the\n",
    "        edge in creating specific solutions to problems. This also means\n",
    "        that we can’t track how they arrive at their solution\n",
    "-   Output layer: a formula to transform the high-level features you\n",
    "    found back into the desired result for this task: classified\n",
    "    samples, regression, etc\n",
    "\n",
    "#### Optimizers\n",
    "\n",
    "-   Root Mean Square Propagation (RMSprop) Optimizer:\n",
    "    -   Adaptive learning rate\n",
    "    -   Better than AdaGrad at dealing with sparse data / saddles\n",
    "    -   Disadvantages: sensitive to choice of hyperparameters and has\n",
    "        optimization issues with non-convex data\n",
    "-   Adam (**ADA**ptive **M**oment Estimation) Optimizer:\n",
    "    -   Adaptive learning rate (RMSprop) AND momentum\n",
    "\n",
    "#### Loss Functions:\n",
    "\n",
    "-   Loss function: formula to determine the difference between the\n",
    "    model’s output and the ground truth\n",
    "    -   Rectified Linear Unit (ReLU):\n",
    "    -   Sigmoid:\n",
    "    -   Softmax:\n",
    "    -   Tversky:\n",
    "        -   \n",
    "\n",
    "#### Other\n",
    "\n",
    "-   Momentum: When calculating next gradient step, past steps are used,\n",
    "    not just the current\n",
    "-   Backpropogation (**Back**ward **propogation** of error):\n",
    "    -   \n",
    "\n",
    "#### References:\n",
    "\n",
    "-   https://www.v7labs.com/blog/deep-learning-guide\n",
    "-   https://en.wikipedia.org/wiki/Tversky_index"
   ],
   "id": "6a49c0ec-e94a-4e7e-a4a9-77bc66a80b5e"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
