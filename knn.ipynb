{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "#### Assumptions:\n",
    "\n",
    "-   non-parametric, supervised learning\n",
    "-   Used for regression and classification tasks\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "-   k: Number of surrounding values to use in classifying each point\n",
    "    Lower k == higher variance, less bias Use an odd k value to avoid\n",
    "    ties\n",
    "\n",
    "-   Distance metric: Euclidean, Manhattan, etc.\n",
    "\n",
    "-   Distributions with more outliers \\>\\> use higher k\n",
    "\n",
    "#### Advantages:\n",
    "\n",
    "-   Simple to understand/use\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "-   Very computationally expensive: all items stored in memory\n",
    "-   Curse of dimensionality: feature selection / dimensionality\n",
    "    reduction recommended prior to use\n",
    "-   Prone to overfitting (due to dimensionality problem, bigger concern\n",
    "    with lower k values)\n",
    "\n",
    "#### Uses:\n",
    "\n",
    "-   Recommendation systems\n",
    "-   Missing value imputation?\n",
    "-   Risk (finance)\n",
    "-   Pattern recognition (text, images)\n",
    "\n",
    "#### References:\n",
    "\n",
    "-   https://www.ibm.com/think/topics/knn"
   ],
   "id": "f53ab022-cfdc-455e-91e4-09ea5ccc9b31"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
